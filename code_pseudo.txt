# @title Run STREAM
import scanpy as sc
import pandas as pd
import json, uuid, textwrap, gzip, base64, io
from string import Template
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import colors
import matplotlib.colors as mplc
import seaborn as sns
import scipy.sparse as sp
import colorcet as cc
import networkx as nx
import decoupler as dc
from pathlib import Path
from datetime import datetime
import ipywidgets as widgets
from IPython.display import display
import sys
import os
import re, glob, subprocess
import warnings
from pandas.errors import PerformanceWarning
from typing import Callable, Optional
from concurrent.futures import ThreadPoolExecutor

# Silence pandas' PerformanceWarning produced during repeated DataFrame inserts
warnings.filterwarnings("ignore", category=PerformanceWarning)

class SecTemplate(Template):
    delimiter = "§"

def add_gene_aliases(top_markers):
    """Append the first human gene alias to each entry's 'names' field (display only).
    Robust to NaN/non-string names and avoids double-tagging.
    """
    alias_file = Path("/content/alias_to_symbol.csv")
    if not alias_file.exists():
        print("Error: alias_to_symbol.csv not found at /content/alias_to_symbol.csv", file=sys.stderr)
        return top_markers

    df = pd.read_csv(alias_file)

    # Build symbol -> [aliases] map (upper-cased keys)
    sym2aliases = {}
    for row in df.itertuples(index=False):
        alias = str(getattr(row, 'alias', '')).strip()
        sym = str(getattr(row, 'symbol', '')).strip().upper()
        if alias and sym and alias.upper() != sym:
            sym2aliases.setdefault(sym, []).append(alias)

    for rec in top_markers:
        name_obj = rec.get('names', None)

        # Skip missing/NaN names
        if name_obj is None:
            continue
        if isinstance(name_obj, float):
            if np.isnan(name_obj):
                continue
            # non-NaN float as gene name is suspicious; skip
            continue

        name_str = str(name_obj).strip()
        if not name_str or name_str.lower() in {"nan", "none"}:
            continue

        # If already tagged like "GENE (alias)", don't re-tag
        base = name_str.split(' (', 1)[0]
        sym = base.upper()

        aliases = sym2aliases.get(sym)
        if aliases and ' (' not in name_str:
            rec['names'] = f"{base} ({aliases[0]})"
        else:
            rec['names'] = base  # ensure consistent display

    return top_markers

# ─────────────────────────────────────────
# Data preparation
# ─────────────────────────────────────────

# choose whether to include all genes or only marker genes in the dashboard
output_options = [
    ("All genes", "all"),
    ("Markers only", "markers"),
]
output_dropdown = widgets.Dropdown(
    options=output_options,
    value="markers",
    description="Gene set:",
)

biology_checkbox = widgets.Checkbox(
    value=False,
    description="Generate biology insights",
)

cite_checkbox = widgets.Checkbox(
    value=False,
    description="CITE-seq (MuData)",
)

# Layer selection -----------------------------------------------------
try:
    layer_options = list(adata.layers.keys())
except AttributeError:
    layer_options = []
    try:
        import muon as mu  # type: ignore
        if isinstance(adata, mu.MuData) and "rna" in adata.mod:
            layer_options = list(adata.mod["rna"].layers.keys())
    except Exception:
        pass
layer_dropdown = widgets.Dropdown(
    options=layer_options,
    value=layer_options[1] if layer_options else None,
    description="Layer:",
)
protein_layer_dropdown = widgets.Dropdown(
    options=[],
    value=None,
    description="Protein layer:",
    disabled=True,
)

def _on_cite_toggle(change):
    if not change["new"]:
        protein_layer_dropdown.options = []
        protein_layer_dropdown.value = None
        protein_layer_dropdown.disabled = True
        return
    try:
        import muon as mu  # type: ignore
        if isinstance(adata, mu.MuData):
            prot = (
                adata.mod.get("prot")
                or adata.mod.get("protein")
                or adata.mod.get("adt")
                or adata.mod.get("ab")
            )
            if prot is not None:
                opts = list(prot.layers.keys())
                protein_layer_dropdown.options = opts
                protein_layer_dropdown.value = opts[0] if opts else None
                protein_layer_dropdown.disabled = False
                return
    except Exception:
        pass
    protein_layer_dropdown.options = []
    protein_layer_dropdown.value = None
    protein_layer_dropdown.disabled = True

cite_checkbox.observe(_on_cite_toggle, names="value")
run_button = widgets.Button(description="Run analysis", button_style="success")
output_area = widgets.Output()
analysis_progress = widgets.IntProgress(value=0, min=0, max=1, description="", bar_style="info")
analysis_step = widgets.HTML("")
display(
    widgets.VBox(
        [
            layer_dropdown,
            protein_layer_dropdown,
            output_dropdown,
            cite_checkbox,
            biology_checkbox,
            run_button,
            analysis_progress,
            analysis_step,
            output_area,
        ]
    )
)
def run_analysis(b):
    global adata
    layer = layer_dropdown.value
    prot_layer = protein_layer_dropdown.value
    use_subset = output_dropdown.value == "markers"
    analysis_progress.max = 6
    analysis_progress.value = 0
    analysis_progress.bar_style = "info"
    analysis_step.value = "Preparing data"
    with output_area:
        output_area.clear_output()
    # Helper to gzip-pack typed arrays and base64 encode them for the template
    def pack_array(arr: np.ndarray) -> str:
        return base64.b64encode(gzip.compress(arr.tobytes())).decode("ascii")

    def pack_quantized_full(arr: np.ndarray, q_lo=1, q_hi=99):
        arr = np.asarray(arr, np.float32).ravel()
        vmin = float(np.nanpercentile(arr, q_lo))
        vmax = float(np.nanpercentile(arr, q_hi))
        rng = max(vmax - vmin, 1e-6)
        q = np.clip(np.round(255.0 * (arr - vmin) / rng), 0, 255).astype(np.uint8)
        return pack_array(q), vmin, vmax

    def pack_json(obj):
        return base64.b64encode(gzip.compress(json.dumps(obj).encode("utf-8"))).decode("ascii")

    adata_local = adata
    protein = None
    if cite_checkbox.value:
        try:
            import muon as mu  # type: ignore
        except ImportError:
            print("CITE-seq selected but muon is not installed; proceeding with RNA-only.")
            cite_checkbox.value = False
        else:
            if not isinstance(adata, mu.MuData):
                print("Error: CITE-seq selected but adata is not a MuData; aborting.")
                return
            rna = adata.mod.get("rna")
            protein = (
                adata.mod.get("prot")
                or adata.mod.get("protein")
                or adata.mod.get("adt")
                or adata.mod.get("ab")
            )
            if rna is None or protein is None:
                print("CITE-seq selected but MuData 'prot' module not found; proceeding with RNA-only.")
                cite_checkbox.value = False
                adata_local = rna if rna is not None else adata
                protein = None
            else:
                adata_local = rna

    adata = adata_local
    analysis_progress.value = 1
    analysis_step.value = "Computing coordinates & clusters"
    # Coordinates & clusters -------------------------------------------------
    umap = adata.obsm["X_umap"].astype(np.float32)
    umap_x = umap[:, 0].ravel()
    umap_y = umap[:, 1].ravel()

    cl_cat = adata.obs[leiden].astype("category")
    cluster_names = cl_cat.cat.categories.astype(str).tolist()
    cluster_codes = cl_cat.cat.codes.to_numpy(dtype=np.uint16)
    # Compute distinct cluster colours using adjacency-aware greedy colouring
    con = adata.obsp.get("connectivities", None)
    clusters = cluster_names
    # size of each cluster for client-side annotation sizing
    cluster_counts = np.bincount(cluster_codes, minlength=len(clusters))
    pal = [mplc.to_hex(cc.glasbey[i % len(cc.glasbey)]) for i in range(len(clusters))]
    if con is not None:
        idx_by_cluster = {c: np.where(cl_cat.cat.codes.values == i)[0] for i, c in enumerate(clusters)}
        G = nx.Graph()
        G.add_nodes_from(clusters)
        for i, ci in enumerate(clusters):
            rows = idx_by_cluster[ci]
            for j in range(i + 1, len(clusters)):
                cj = clusters[j]
                cols = idx_by_cluster[cj]
                w = con[rows[:, None], cols].sum()
                if float(w) > 0:
                    G.add_edge(ci, cj)
        color_index = {}
        available = list(range(len(pal)))
        order = sorted(G.degree, key=lambda x: x[1], reverse=True)
        for node, _deg in order:
            for k in available:
                if all(color_index.get(nbr) != k for nbr in G.neighbors(node)):
                    color_index[node] = k
                    available.remove(k)
                    break
        cluster_colors_map = {c: pal[color_index[c]] for c in clusters}
    else:
        order = np.argsort(-cluster_counts)
        cluster_colors_map = {clusters[i]: pal[pos] for pos, i in enumerate(order)}

    cells = adata.obs_names.astype(str).tolist()


    analysis_progress.value = 2
    analysis_step.value = "Computing composition data"
    # Composition data ------------------------------------------------------
    sample_key_candidates = [
        "sample",
        "sample_id",
        "sampleid",
        "donor",
        "patient",
        "channel",
        "batch",
        "group",
        "treatment",
        "condition",
    ]
    sample_key = next(
        (
            c
            for c in adata.obs.columns
            if any(k in c.lower() for k in sample_key_candidates)
        ),
        None,
    )

    obs_cats = sorted([c for c in adata.obs.select_dtypes(["category"]).columns])

    # Include common sample/group/treatment columns even if not categorical
    extra_obs = [
        c
        for c in adata.obs.columns
        if c not in obs_cats
        and any(
            key in c.lower()
            for key in ["sample", "group", "treat", "cond", "batch"]
        )
    ]
    for c in extra_obs:
        adata.obs[c] = adata.obs[c].astype("category")
    obs_cats = sorted(obs_cats + extra_obs)
    obs_levels = {
        c: adata.obs[c].cat.categories.astype(str).tolist() for c in obs_cats
    }
    obs_colors = {}
    for c in obs_cats:
        col = adata.uns.get(f"{c}_colors")
        if col is not None and len(col) == len(obs_levels[c]):
            obs_colors[c] = {
                lvl: color for lvl, color in zip(obs_levels[c], col)
            }

    sample_labels: list[str] = []
    composition = None
    if sample_key is not None:
        sample_cat = adata.obs[sample_key].astype("category")
        sample_labels = sample_cat.cat.categories.astype(str).tolist()
        sample_codes = sample_cat.cat.codes.to_numpy(dtype=np.uint16)
        n_samples = len(sample_labels)
        n_groups = len(cluster_names)
        comp_counts = np.zeros((n_samples, n_groups), dtype=np.int32)
        for s, g in zip(sample_codes, cluster_codes):
            if s >= 0:
                comp_counts[s, g] += 1
        row_sums = comp_counts.sum(axis=1, keepdims=True)
        row_sums[row_sums == 0] = 1
        props = comp_counts / row_sums
        composition = {
            "sample_key": sample_key,
            "sample_labels": sample_labels,
            "groupby": leiden,
            "group_labels": cluster_names,
            "counts": pack_array(comp_counts.ravel().astype(np.int32)),
            "props": pack_array(props.astype(np.float32).ravel()),
            "n_samples": int(n_samples),
            "n_groups": int(n_groups),
        }

    obs_col_matrix = {}
    for c in obs_cats:
        codes = adata.obs[c].cat.codes.to_numpy(dtype=np.uint16)
        obs_col_matrix[c] = pack_array(codes)


    analysis_progress.value = 3
    analysis_step.value = "Finding marker genes"
    # ─────────────────────────────────────────
    # Marker genes (top 20 per cluster)
    # ─────────────────────────────────────────
    marker_dict = {}
    markers_template = {}
    if protein is None:
        methods = ["wilcoxon", "t-test"]
        for m in methods:
            key = f"{m}_rank_genes_groups"
            sc.tl.rank_genes_groups(
                adata,
                use_raw=False,
                groupby=leiden,
                method=m,
                n_genes=adata.raw.shape[1] if adata.raw is not None else adata.shape[1],
                corr_method="bonferroni",
                layer=layer,
                key_added=key,
                max_iter=2000,
            )
            clust_dict = {}
            for c in adata.uns[key]["names"].dtype.names:
                df = sc.get.rank_genes_groups_df(adata, group=c, key=key)
                if "pvals_adj" in df.columns:
                    df = df[df["pvals_adj"] < 0.05]
                elif "pvals" in df.columns:
                    df = df[df["pvals"] < 0.05]
                df = df.sort_values("scores", ascending=False).head(50)
                cols = [
                    x
                    for x in ["names", "scores", "logfoldchanges", "pvals_adj", "pvals"]
                    if x in df.columns
                ]
                clust_dict[c] = df[cols].to_dict("records")
            marker_dict[m] = clust_dict
        markers_template = marker_dict
    else:
        # RNA markers (Wilcoxon only)
        key = "wilcoxon_rank_genes_groups"
        sc.tl.rank_genes_groups(
            adata,
            use_raw=False,
            groupby=leiden,
            method="wilcoxon",
            n_genes=adata.raw.shape[1] if adata.raw is not None else adata.shape[1],
            corr_method="bonferroni",
            layer=layer,
            key_added=key,
            max_iter=2000,
        )
        clust_dict_rna = {}
        for c in adata.uns[key]["names"].dtype.names:
            df = sc.get.rank_genes_groups_df(adata, group=c, key=key)
            if "pvals_adj" in df.columns:
                df = df[df["pvals_adj"] < 0.05]
            elif "pvals" in df.columns:
                df = df[df["pvals"] < 0.05]
            df = df.sort_values("scores", ascending=False).head(50)
            cols = [
                x
                for x in ["names", "scores", "logfoldchanges", "pvals_adj", "pvals"]
                if x in df.columns
            ]
            clust_dict_rna[c] = df[cols].to_dict("records")

        # Protein markers (Wilcoxon only)
        key_p = "prot_wilcoxon_rank_genes_groups"
        sc.tl.rank_genes_groups(
            protein,
            use_raw=False,
            groupby=leiden,
            method="wilcoxon",
            n_genes=protein.raw.shape[1] if protein.raw is not None else protein.shape[1],
            corr_method="bonferroni",
            layer=prot_layer,
            key_added=key_p,
            max_iter=2000,
        )
        clust_dict_prot = {}
        for c in protein.uns[key_p]["names"].dtype.names:
            df = sc.get.rank_genes_groups_df(protein, group=c, key=key_p)
            if "pvals_adj" in df.columns:
                df = df[df["pvals_adj"] < 0.05]
            elif "pvals" in df.columns:
                df = df[df["pvals"] < 0.05]
            df = df.sort_values("scores", ascending=False).head(50)
            cols = [
                x
                for x in ["names", "scores", "logfoldchanges", "pvals_adj", "pvals"]
                if x in df.columns
            ]
            clust_dict_prot[c] = df[cols].to_dict("records")

        marker_dict = {"wilcoxon": clust_dict_rna}
        markers_template = {"RNA": clust_dict_rna, "Protein": clust_dict_prot}

    analysis_progress.value = 4
    analysis_step.value = "Identifying neighborhood-aware markers"
    # Neighborhood-aware markers ------------------------------------------------
    top_markers_dict = {}
    top_markers_template = {}

    def compute_top_markers_df(
        adata, leiden, top_n=50, pval_threshold=0.05, layer=None, neighbor_k=2
    ):
        methods = ["wilcoxon"]
        rank_keys = {}

        if leiden not in adata.obs:
            raise ValueError("Column 'leiden' not found in rna.obs")
        if layer is not None and layer not in adata.layers:
            raise ValueError(f"Layer '{layer}' not found in adata.layers")
        if "connectivities" not in adata.obsp:
            if "X_pca" not in adata.obsm:
                print("Warning: X_pca not found. Computing PCA.")
                sc.tl.pca(adata, svd_solver="arpack")
            print("Warning: Neighbor graph not found. Computing neighbors.")
            sc.pp.neighbors(adata, use_rep="X_pca")

        cluster_ids = adata.obs[leiden].astype(str)
        clusters = np.sort(cluster_ids.unique())
        n_clusters = len(clusters)

        neighbor_k = min(neighbor_k, n_clusters - 1)
        if neighbor_k <= 0:
            for method in methods:
                key = f"{method}_rank_genes_groups"
                sc.tl.rank_genes_groups(
                    adata,
                    groupby=leiden,
                    method=method,
                    n_genes=adata.raw.shape[1]
                    if adata.raw is not None
                    else adata.shape[1],
                    corr_method="bonferroni",
                    layer=layer,
                    key_added=key,
                    reference="rest",
                    max_iter=2000,
                )
                rank_keys[method] = [key]
        else:
            conn_sparse = adata.obsp["connectivities"]
            cat = pd.Categorical(cluster_ids, categories=clusters)
            n_cells = len(cluster_ids)
            cluster_mask = sp.csr_matrix(
                (np.ones(n_cells), (np.arange(n_cells), cat.codes)),
                shape=(n_cells, n_clusters),
            )
            connectivity_matrix = (
                cluster_mask.T @ conn_sparse @ cluster_mask
            ).toarray()
            np.fill_diagonal(connectivity_matrix, 0)
            neighbor_clusters = {
                clusters[i]: clusters[np.argsort(connectivity_matrix[i])[::-1][:neighbor_k]]
                for i in range(n_clusters)
            }

            for method in methods:
                method_keys = []
                for cluster in clusters:
                    neighbors = neighbor_clusters[cluster]

                    temp_group = pd.Series([
                        "ignore"
                    ] * adata.shape[0], index=adata.obs_names)
                    temp_group[cluster_ids.isin(neighbors)] = "temp_neighbors"
                    temp_group[cluster_ids == cluster] = cluster
                    temp_group = temp_group.astype("category")
                    adata.obs["temp_group"] = temp_group

                    key = f"{method}_rank_genes_groups_{cluster}"
                    sc.tl.rank_genes_groups(
                        adata,
                        groupby="temp_group",
                        groups=[cluster],
                        reference="temp_neighbors",
                        method=method,
                        n_genes=adata.raw.shape[1]
                        if adata.raw is not None
                        else adata.shape[1],
                        corr_method="bonferroni",
                        layer=layer,
                        key_added=key,
                        max_iter=2000,
                    )
                    method_keys.append(key)

                rank_keys[method] = method_keys
                adata.obs.drop(columns=["temp_group"], inplace=True)

        records = []
        for key in rank_keys["wilcoxon"]:
            cluster = key.rsplit("_", 1)[-1]
            try:
                df = sc.get.rank_genes_groups_df(adata, group=cluster, key=key)
                df = df[df["pvals_adj"] <= pval_threshold]
                if "logfoldchanges" in df.columns:
                    df["abs_logfoldchanges"] = df["logfoldchanges"].abs()
                    df = df.sort_values("abs_logfoldchanges", ascending=False)
            except Exception:
                names_raw = adata.uns[key]["names"][cluster]
                scores_raw = adata.uns[key].get("scores", {}).get(cluster)
                logfc_raw = adata.uns[key].get("logfoldchanges", {}).get(cluster)
                pvals_adj_raw = adata.uns[key].get("pvals_adj", {}).get(cluster)
                df = pd.DataFrame(
                    {
                        "names": names_raw,
                        "scores": scores_raw,
                        "logfoldchanges": logfc_raw,
                        "pvals_adj": pvals_adj_raw,
                    }
                )
                df = df[df["pvals_adj"] <= pval_threshold]
                if "logfoldchanges" in df.columns:
                    df["abs_logfoldchanges"] = df["logfoldchanges"].abs()
                    df = df.sort_values("abs_logfoldchanges", ascending=False)

            df = df.head(top_n)
            df[leiden] = cluster
            cols = [
                "names",
                "scores",
                "logfoldchanges",
                "abs_logfoldchanges",
                "pvals_adj",
                leiden,
            ]
            cols = [c for c in cols if c in df.columns]
            records.append(df[cols])

        return pd.concat(records, ignore_index=True)

    if protein is None:
        top_markers_df = compute_top_markers_df(adata, leiden, layer=layer)
        if leiden in top_markers_df.columns:
            for c, df in top_markers_df.groupby(leiden):
                if "abs_logfoldchanges" in df.columns:
                    df = df.sort_values("abs_logfoldchanges", ascending=False)
                cols = [
                    x
                    for x in [
                        "names",
                        "scores",
                        "logfoldchanges",
                        "abs_logfoldchanges",
                        "pvals_adj",
                        "pvals",
                    ]
                    if x in df.columns
                ]
                top_markers_dict[c] = df.head(50)[cols].to_dict("records")
        else:
            cluster_cols = [
                c
                for c in top_markers_df.columns
                if not any(
                    c.startswith(p)
                    for p in ["scores_", "logfoldchanges_", "pvals_adj_", "pvals_"]
                )
            ]
            for clust in cluster_cols:
                df = pd.DataFrame({"names": top_markers_df[clust]})
                for stat in [
                    "scores",
                    "logfoldchanges",
                    "abs_logfoldchanges",
                    "pvals_adj",
                    "pvals",
                ]:
                    col = f"{stat}_{clust}"
                    if col in top_markers_df.columns:
                        df[stat] = top_markers_df[col]
                if "abs_logfoldchanges" in df.columns:
                    df = df.sort_values("abs_logfoldchanges", ascending=False)
                cols = [
                    x
                    for x in [
                        "names",
                        "scores",
                        "logfoldchanges",
                        "abs_logfoldchanges",
                        "pvals_adj",
                        "pvals",
                    ]
                    if x in df.columns
                ]
                top_markers_dict[clust] = df.head(50)[cols].to_dict("records")
        top_markers_template = top_markers_dict
    else:
        top_markers_df_rna = compute_top_markers_df(adata, leiden, layer=layer)
        try:
            top_markers_df_prot = compute_top_markers_df(protein, leiden, layer=prot_layer)
        except Exception:
            top_markers_df_prot = pd.DataFrame()
        top_markers_dict_rna = {}
        if leiden in top_markers_df_rna.columns:
            for c, df in top_markers_df_rna.groupby(leiden):
                if "abs_logfoldchanges" in df.columns:
                    df = df.sort_values("abs_logfoldchanges", ascending=False)
                cols = [
                    x
                    for x in [
                        "names",
                        "scores",
                        "logfoldchanges",
                        "abs_logfoldchanges",
                        "pvals_adj",
                        "pvals",
                    ]
                    if x in df.columns
                ]
                top_markers_dict_rna[c] = df.head(50)[cols].to_dict("records")
        else:
            cluster_cols = [
                c
                for c in top_markers_df_rna.columns
                if not any(
                    c.startswith(p)
                    for p in ["scores_", "logfoldchanges_", "pvals_adj_", "pvals_"]
                )
            ]
            for clust in cluster_cols:
                df = pd.DataFrame({"names": top_markers_df_rna[clust]})
                for stat in [
                    "scores",
                    "logfoldchanges",
                    "abs_logfoldchanges",
                    "pvals_adj",
                    "pvals",
                ]:
                    col = f"{stat}_{clust}"
                    if col in top_markers_df_rna.columns:
                        df[stat] = top_markers_df_rna[col]
                if "abs_logfoldchanges" in df.columns:
                    df = df.sort_values("abs_logfoldchanges", ascending=False)
                cols = [
                    x
                    for x in [
                        "names",
                        "scores",
                        "logfoldchanges",
                        "abs_logfoldchanges",
                        "pvals_adj",
                        "pvals",
                    ]
                    if x in df.columns
                ]
                top_markers_dict_rna[clust] = df.head(50)[cols].to_dict("records")
        top_markers_dict_prot = {}
        if not top_markers_df_prot.empty:
            if leiden in top_markers_df_prot.columns:
                for c, df in top_markers_df_prot.groupby(leiden):
                    if "abs_logfoldchanges" in df.columns:
                        df = df.sort_values("abs_logfoldchanges", ascending=False)
                    cols = [
                        x
                        for x in [
                            "names",
                            "scores",
                            "logfoldchanges",
                            "abs_logfoldchanges",
                            "pvals_adj",
                            "pvals",
                        ]
                        if x in df.columns
                    ]
                    top_markers_dict_prot[c] = df.head(50)[cols].to_dict("records")
            else:
                cluster_cols = [
                    c
                    for c in top_markers_df_prot.columns
                    if not any(
                        c.startswith(p)
                        for p in ["scores_", "logfoldchanges_", "pvals_adj_", "pvals_"]
                    )
                ]
                for clust in cluster_cols:
                    df = pd.DataFrame({"names": top_markers_df_prot[clust]})
                    for stat in [
                        "scores",
                        "logfoldchanges",
                        "abs_logfoldchanges",
                        "pvals_adj",
                        "pvals",
                    ]:
                        col = f"{stat}_{clust}"
                        if col in top_markers_df_prot.columns:
                            df[stat] = top_markers_df_prot[col]
                    if "abs_logfoldchanges" in df.columns:
                        df = df.sort_values("abs_logfoldchanges", ascending=False)
                    cols = [
                        x
                        for x in [
                            "names",
                            "scores",
                            "logfoldchanges",
                            "abs_logfoldchanges",
                            "pvals_adj",
                            "pvals",
                        ]
                        if x in df.columns
                    ]
                    top_markers_dict_prot[clust] = df.head(50)[cols].to_dict("records")
        top_markers_dict = top_markers_dict_rna
        top_markers_template = {"RNA": top_markers_dict_rna, "Protein": top_markers_dict_prot}

    # Cell type enrichment -----------------------------------------------------
    markers = dc.op.resource("PanglaoDB", organism="human")
    markers = markers[
        markers["human"].astype(bool)
        & markers["canonical_marker"].astype(bool)
        & (markers["human_sensitivity"].astype(float) > 0.5)
    ]
    markers = markers[~markers.duplicated(["cell_type", "genesymbol"])]
    markers = markers.rename(columns={"cell_type": "source", "genesymbol": "target"})
    markers = markers[["source", "target"]]
    dc.mt.ulm(data=adata, net=markers, tmin=3)
    score = dc.pp.get_obsm(adata, key="score_ulm")
    ctype_rank_df = dc.tl.rankby_group(adata=score, groupby=leiden, reference="rest", method="t-test_overestim_var")
    ctype_rank_df = ctype_rank_df[ctype_rank_df["stat"] > 0]
    n_ctypes = 5
    ctypes_dict = (
        ctype_rank_df.groupby("group", observed=False)
        .head(n_ctypes)
        .groupby("group", observed=False)["name"]
        .apply(list)
        .to_dict()
    )
    all_ctypes = sorted({ct for lst in ctypes_dict.values() for ct in lst})
    enrich_dict = {}
    score_df = score.to_df() if hasattr(score, "to_df") else pd.DataFrame(score.X, index=score.obs_names, columns=score.var_names)
    for ct in all_ctypes:
        if ct in score_df.columns:
            b64, vmin, vmax = pack_quantized_full(score_df[ct].astype(np.float32).to_numpy())
            enrich_dict[ct] = {"data": b64, "vmin": vmin, "vmax": vmax}

    # Transcription factor enrichment ---------------------------------------
    collectri = dc.op.collectri(organism="human")
    dc.mt.ulm(data=adata, net=collectri)
    score_tf = dc.pp.get_obsm(adata=adata, key="score_ulm")
    df_tf = dc.tl.rankby_group(adata=score_tf, groupby=leiden, reference="rest", method="t-test_overestim_var")
    df_tf = df_tf[df_tf["stat"] > 0]
    n_markers = 5
    source_markers = (
        df_tf.groupby("group", observed=False)
        .head(n_markers)
        .drop_duplicates("name")
        .groupby("group", observed=False)["name"]
        .apply(lambda x: list(x))
        .to_dict()
    )
    all_tfs = sorted({tf for lst in source_markers.values() for tf in lst})
    tf_enrich_dict = {}
    score_tf_df = score_tf.to_df() if hasattr(score_tf, "to_df") else pd.DataFrame(score_tf.X, index=score_tf.obs_names, columns=score_tf.var_names)
    for tf in all_tfs:
        if tf in score_tf_df.columns:
            b64, vmin, vmax = pack_quantized_full(score_tf_df[tf].astype(np.float32).to_numpy())
            tf_enrich_dict[tf] = {"data": b64, "vmin": vmin, "vmax": vmax}

    # Union of marker genes --------------------------------------------------
    genes_union = set()
    for method in marker_dict.values():
        for records in method.values():
            for r in records:
                genes_union.add(r["names"])
    for records in top_markers_dict.values():
        for r in records:
            genes_union.add(r["names"])
    if not genes_union:
        genes_union = set(adata.var_names)

    genes_for_html = (
    {_base_gene(g) for g in genes_union} if use_subset else set(adata.var_names))
    # Ensure stable ordering even if gene identifiers contain non-strings
    genes_for_html = sorted(map(str, genes_for_html))
    adata_subset = adata[:, genes_for_html].copy()

    # Gene‑wise expression vectors + vmin/vmax (20‑99 % quantiles) -----------
    expr_mat = adata_subset.layers[layer].astype(np.float32)
    n_cells = adata_subset.shape[0]
    exp_dict = {}
    vmin_arr = np.zeros(len(genes_for_html), dtype=np.float32)
    vmax_arr = np.zeros(len(genes_for_html), dtype=np.float32)
    for gi, gene in enumerate(genes_for_html):
        idx = adata_subset.var_names.get_loc(gene)
        vec = expr_mat[:, idx]
        if sp.issparse(vec):
            vec = vec.toarray().ravel()
        else:
            vec = np.asarray(vec).ravel()
        nz = np.nonzero(vec)[0]
        if nz.size:
            subset = vec[nz]
            vmin = float(np.percentile(subset, 20).astype(np.float32))
            vmax = float(np.percentile(subset, 99).astype(np.float32))
            vmin_arr[gi] = vmin
            vmax_arr[gi] = vmax
            rng = max(vmax - vmin, 1e-6)
            q = np.clip(np.round(255.0 * (subset - vmin) / rng), 0, 255).astype(np.uint8)
            exp_dict[gene] = {"i": nz.astype(np.int32).tolist(), "x": q.tolist()}
        else:
            vmin_arr[gi] = vmax_arr[gi] = 0.0
            exp_dict[gene] = {"i": [], "x": []}

    exp_json = json.dumps(exp_dict).encode("utf-8")
    exp_b64 = base64.b64encode(gzip.compress(exp_json)).decode("ascii")
    vmin_b64 = pack_array(vmin_arr)
    vmax_b64 = pack_array(vmax_arr)

    # Protein expression (ADT) -----------------------------------------------
    if protein is not None:
        layer_name = prot_layer
        if layer_name is None:
            layer_name = (
                "denoised_protein"
                if "denoised_protein" in protein.layers
                else (next(iter(protein.layers.keys())) if protein.layers else None)
            )
        prot_counts = (
            protein.layers[layer_name] if layer_name else protein.X
        ).astype(np.float32)
        prot_genes = list(map(str, protein.var_names))
        prot_exp_dict = {}
        prot_vmin_arr = np.zeros(len(prot_genes), dtype=np.float32)
        prot_vmax_arr = np.zeros(len(prot_genes), dtype=np.float32)
        for pi, gene in enumerate(prot_genes):
            vec = prot_counts[:, pi]
            if sp.issparse(vec):
                vec = vec.toarray().ravel()
            else:
                vec = np.asarray(vec).ravel()
            nz = np.nonzero(vec)[0]
            if nz.size:
                subset = vec[nz]
                vmin = float(np.percentile(subset, 20).astype(np.float32))
                vmax = float(np.percentile(subset, 99).astype(np.float32))
                prot_vmin_arr[pi] = vmin
                prot_vmax_arr[pi] = vmax
                rng = max(vmax - vmin, 1e-6)
                q = np.clip(np.round(255.0 * (subset - vmin) / rng), 0, 255).astype(np.uint8)
                prot_exp_dict[gene] = {"i": nz.astype(np.int32).tolist(), "x": q.tolist()}
            else:
                prot_vmin_arr[pi] = prot_vmax_arr[pi] = 0.0
                prot_exp_dict[gene] = {"i": [], "x": []}
        prot_exp_b64 = pack_json(prot_exp_dict)
        prot_genes_b64 = pack_json(prot_genes)
        prot_vmin_b64 = pack_array(prot_vmin_arr)
        prot_vmax_b64 = pack_array(prot_vmax_arr)
    else:
        prot_exp_b64 = pack_json({})
        prot_genes_b64 = pack_json([])
        prot_vmin_b64 = pack_array(np.zeros(0, dtype=np.float32))
        prot_vmax_b64 = pack_array(np.zeros(0, dtype=np.float32))

    # ─────────────────────────────────────────
    # Custom colour map – GREY → RAINBOW
    # ─────────────────────────────────────────
    grey_ramp = plt.cm.Greys_r(np.linspace(0.85, 0.15, 30))
    rainbow = cc.cm.rainbow_bgyr_35_85_c72(np.linspace(0.20, 1.00, 226))
    colorsComb = np.vstack([grey_ramp, rainbow])
    mymap = colors.LinearSegmentedColormap.from_list("my_colormap", colorsComb, N=256)

    palette = [colors.to_hex(mymap(x), keep_alpha=False) for x in np.linspace(0, 1, 256)]

    for method in marker_dict:
        for cluster in marker_dict[method]:
            add_gene_aliases(marker_dict[method][cluster])
    for cluster in top_markers_dict:
        add_gene_aliases(top_markers_dict[cluster])
    # Pseudobulk differential expression summary
    write_pseudobulk_and_metadata(
        adata,
        celltype_var="celltype",
        Individual_var="individual",
        Group_var="disease",
        outdir="Pseudobulk",
    )
    Path("EdgeR_pipeline.R").write_text(EDGE_R_SCRIPT)
    summary = run_edger_batch(
        folder="Pseudobulk",
        rscript_path="EdgeR_pipeline.R",
        sample_col="Sample",
        group_col="disease",
        relevel="normal",
        contrast_groups=("COVID-19", "normal"),
    )
    deg_plot_b64 = plot_deg_counts(summary)

    analysis_progress.value = 5
    analysis_step.value = "Building dashboard"
    # ─────────────────────────────────────────
    # Fill HTML template
    # ─────────────────────────────────────────
    template_path = Path("template_pseudo.html")
    with template_path.open(encoding="utf-8") as f:
        html = SecTemplate(f.read())

    ts = datetime.now()
    filename = getattr(adata, 'filename', None)
    dataset_name = project_title
    timestamp = ts.strftime("%Y%m%d-%H%M%S")
    generated = ts.isoformat(timespec='seconds')

    subs = {
        "DOM": "dash_" + uuid.uuid4().hex,
        "EXP": json.dumps(exp_b64),
        "GENES": json.dumps(pack_json(list(adata_subset.var_names))),
        "CMAP": json.dumps(palette),
        "VMIN": json.dumps(vmin_b64),
        "VMAX": json.dumps(vmax_b64),
        "PROT_EXP": json.dumps(prot_exp_b64),
        "PROT_GENES": json.dumps(prot_genes_b64),
        "PROT_VMIN": json.dumps(prot_vmin_b64),
        "PROT_VMAX": json.dumps(prot_vmax_b64),
        "CELLS": json.dumps(pack_json(cells)),
        "MARKERS": json.dumps(pack_json(markers_template)),
        "TOP_MARKERS": json.dumps(pack_json(top_markers_template)),
        "CTYPES": json.dumps(pack_json(ctypes_dict)),
        "ENRICH": json.dumps(enrich_dict),
        "TF_MARKERS": json.dumps(source_markers),
        "TF_SCORES": json.dumps(tf_enrich_dict),
        "LEIDEN": json.dumps(leiden),
        "UMAP_X": json.dumps(pack_array(umap_x)),
        "UMAP_Y": json.dumps(pack_array(umap_y)),
        "CLUSTER_CODES": json.dumps(pack_array(cluster_codes)),
        "CLUSTER_NAMES": json.dumps(pack_json(cluster_names)),
        "CLUSTER_COLORS_MAP": json.dumps(cluster_colors_map),
        "CLUSTER_COUNTS": json.dumps(pack_json(cluster_counts.tolist())),
        "DATA_NAME": dataset_name,
        "LAYER": layer,
        "GENERATED": generated,
        "DEG_PLOT": deg_plot_b64,
    }
    subs["TOOL_LICENSE"] = "MIT License"
    subs["DATA_LICENSE"] = "CC BY 4.0 (Example – replace per dataset)"

    # QC metrics ------------------------------------------------------------
    def obs_or_none(col, dtype=np.float32):
        if col in adata.obs:
            return pack_array(adata.obs[col].to_numpy(dtype=dtype))
        return None

    subs["NCOUNTS"] = json.dumps(obs_or_none("n_counts"))
    subs["NGENES"] = json.dumps(obs_or_none("n_genes"))
    subs["PCT_MT"] = json.dumps(obs_or_none("pct_counts_mt"))
    subs["PCT_RIBO"] = json.dumps(obs_or_none("pct_counts_ribo"))
    subs["DBL_SCORE"] = json.dumps(obs_or_none("doublet_score"))

    if "batch" in adata.obs:
        subs["BATCH"] = json.dumps(adata.obs["batch"].astype(str).tolist())
    else:
        subs["BATCH"] = json.dumps(None)

    if "cell_cycle" in adata.obs:
        subs["CELL_CYCLE"] = json.dumps(adata.obs["cell_cycle"].astype(str).tolist())
    else:
        subs["CELL_CYCLE"] = json.dumps(None)

    if "connectivities" in adata.obsp:
        con = adata.obsp["connectivities"].tocsr()
        subs["NN_INDPTR"] = json.dumps(pack_array(con.indptr.astype(np.int32)))
        subs["NN_INDICES"] = json.dumps(pack_array(con.indices.astype(np.int32)))
    else:
        subs["NN_INDPTR"] = json.dumps(None)
        subs["NN_INDICES"] = json.dumps(None)

    subs["OBS_CATS"] = json.dumps(obs_cats)
    subs["OBS_LEVELS"] = json.dumps(obs_levels)
    subs["OBS_COLORS"] = json.dumps(obs_colors)
    subs["SAMPLE_KEY"] = json.dumps(sample_key)
    subs["COMPOSITION"] = json.dumps(composition)
    subs["DIRICHLET"] = json.dumps(None)
    subs["OBS_COL_MATRIX"] = json.dumps(obs_col_matrix)

    insights_md = ""
    biology_info: dict[str, dict[str, object]] = {}
    if biology_checkbox.value:
        cluster_sizes_df = (adata.obs[leiden].astype(str).value_counts().sort_index().reset_index())
        cluster_sizes_df.columns = ['cluster', 'n_cells']
        cluster_sizes_csv = cluster_sizes_df.to_csv(index=False)
        ctype_rank_csv = ctype_rank_df.rename(
            columns={"group": "cluster", "name": "cell_type"}
        ).to_csv(index=False)
        total_clusters = len(cluster_sizes_df)

        def _insight_progress(i, cluster, total=total_clusters):
            analysis_step.value = (
                f"Building dashboard: querying AI model for cluster {cluster} ({i}/{total})"
            )

        insights_md = generate_biology_insights(
            leiden,
            dataset_name,
            cluster_sizes_csv,
            json.dumps(top_markers_template),
            json.dumps(marker_dict),
            ctype_rank_csv,
            json.dumps(source_markers),
            adata.uns.get("tissue_context", "") if hasattr(adata, "uns") else "",
            progress_cb=_insight_progress,
        ) or ""
        analysis_step.value = "Building dashboard"

        # Parse the harmonised Markdown blocks into a structured dictionary.
        # Use a line-oriented approach to tolerate optional blank lines and
        # differing newline styles.
        current: str | None = None
        for line in insights_md.splitlines():
            line = line.strip()
            if line.startswith(f"### {leiden} "):
                current = line.split(f"### {leiden} ", 1)[1].strip()
                biology_info[current] = {}
                continue
            if not current:
                continue
            if line.startswith("Canonical Label:"):
                biology_info[current]["canonical_label"] = line.split(":", 1)[1].strip()
            elif line.startswith("Fine Label:"):
                biology_info[current]["fine_label"] = line.split(":", 1)[1].strip()
            elif line.startswith("Confidence:"):
                biology_info[current]["confidence"] = line.split(":", 1)[1].strip()
            elif line.startswith("Key Markers:"):
                markers = line.split(":", 1)[1]
                biology_info[current]["key_markers"] = [
                    m.strip() for m in markers.split(",") if m.strip()
                ]

        for info in biology_info.values():
            info.setdefault("canonical_label", "")
            info.setdefault("fine_label", "")
            info.setdefault("confidence", "")
            info.setdefault("key_markers", [])

    subs["BIOLOGY_MD"] = json.dumps(pack_json(insights_md))
    subs["BIOLOGY_INFO"] = json.dumps(pack_json(biology_info))

    out_file = Path(f"{dataset_name}_{layer}_{timestamp}.html")
    with out_file.open("w", encoding="utf-8") as f:
        f.write(textwrap.dedent(html.substitute(subs)))

    print(f"✅ Dashboard saved → {out_file.resolve()}")

    analysis_progress.value = 6
    analysis_step.value = "Analysis complete"
    analysis_progress.bar_style = "success"

    # Flush out variables created during the pipeline run to free memory
    # while keeping the main AnnData object available for subsequent use.
    keep_vars = {"adata"}
    for var in list(locals()):
        if var not in keep_vars:
            try:
                del locals()[var]
            except KeyError:
                pass
    import gc
    gc.collect()
run_button.on_click(run_analysis)

# ─────────────────────────────────────────
# Biology insights (Google Gemini helper)
# ─────────────────────────────────────────

# Biology insights (OpenAI + Google Gemini helper) — ROBUST VERSION
# Replaces the simpler try/except in code.txt
import google.generativeai as genai
import requests, sys, time, math
from typing import Dict, List, Tuple, Optional

try:
    from openai import OpenAI
except Exception:
    OpenAI = None


def to_markdown(text: str) -> str:
    """Return markdown-formatted text (kept as pass-through)."""
    return text or ""


def _base_gene(n):
    s = str(n)
    i = s.find(' (')
    return s[:i] if i >= 0 else s


# --- NEW: model registry with input/output limits and priority bucket ------
# Keep only text-capable chat models here (omit Imagen/Veo/Embeddings).
# You can adjust the order in PREFERENCE to bias towards quality vs speed.
MODEL_REGISTRY: Dict[str, Dict[str, int]] = {
    "gemini-2.5-pro":                {"in": 1048576, "out": 65536},
    "gemini-2.5-flash":              {"in": 1048576, "out": 65536},
    "gemini-1.5-pro-latest":         {"in": 2000000, "out": 8192},
    "gemini-1.5-pro-002":            {"in": 2000000, "out": 8192},
    "gemini-1.5-flash-latest":       {"in": 1000000, "out": 8192},
    "gemini-1.5-flash-002":          {"in": 1000000, "out": 8192},
    "gemini-1.5-flash-8b-latest":    {"in": 1000000, "out": 8192},
    # Optional extras you can enable later:
    # "gemini-2.0-flash":            {"in": 1048576, "out": 8192},
}

# Preference tiers (earlier is preferred when limits allow).
PREFERENCE: List[str] = [
    "gemini-2.5-pro",
    "gemini-2.5-flash",
    "gemini-1.5-pro-latest",
    "gemini-1.5-flash-latest",
    "gemini-1.5-flash-8b-latest",
]


def _approx_tokens(s: str) -> int:
    """Crude token estimate (Gemini tokens ~= 4 chars in English)."""
    return max(1, math.ceil(len(s) / 4))


def _extract_text(resp) -> Optional[str]:
    """Be defensive: .text if present; otherwise walk candidates/parts."""
    if not resp:
        return None
    # Common happy path
    txt = getattr(resp, "text", None)
    if isinstance(txt, str) and txt.strip():
        return txt
    # Candidates → content → parts → text
    try:
        cands = getattr(resp, "candidates", None) or []
        for c in cands:
            content = getattr(c, "content", None)
            if content and getattr(content, "parts", None):
                for p in content.parts:
                    t = getattr(p, "text", None)
                    if isinstance(t, str) and t.strip():
                        return t
    except Exception:
        pass
    return None


def _sleep_backoff(try_i: int):
    """Exponential backoff with jitter."""
    # 1, 2, 4, 8, 16 sec (capped)
    delay = min(16.0, 1.0 * (2 ** try_i))
    time.sleep(delay + 0.2 * (try_i + 1))


def _is_prompt_too_long_error(e: Exception) -> bool:
    msg = str(e).lower()
    return ("too long" in msg) or ("exceeds" in msg and "token" in msg) or ("prompt" in msg and "length" in msg)


def _is_rate_or_quota_error(e: Exception) -> bool:
    msg = str(e).lower()
    return ("429" in msg) or ("rate" in msg) or ("quota" in msg) or ("resourceexhausted" in msg)


def _is_transient_server_error(e: Exception) -> bool:
    msg = str(e).lower()
    return (
        ("5xx" in msg)
        or ("internal" in msg)
        or ("unavailable" in msg)
        or ("deadline" in msg)
        or ("timeout" in msg)
        or ("timed out" in msg)
    )


def _eligible_models_for_prompt(prompt_tokens: int) -> List[str]:
    """Return candidate models whose input limit can safely accommodate the prompt."""
    # Keep ~15% headroom for safety/system/tokens produced by API.
    need = int(prompt_tokens * 1.15)
    eligible = [m for m in PREFERENCE if MODEL_REGISTRY.get(m, {}).get("in", 0) >= need]
    # If none is eligible, still return all (we will try & handle 'too long').
    return eligible or PREFERENCE[:]


def _try_openai(prompt: str,
                temperature: float,
                max_output_tokens: Optional[int],
                timeout_seconds: int) -> Optional[str]:
    """Attempt to generate text using OpenAI; return ``None`` on failure."""
    if OpenAI is None:
        print("[insights] OpenAI package not installed; skipping.", file=sys.stderr)
        return None
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("[insights] OPENAI_API_KEY not set; skipping OpenAI.", file=sys.stderr)
        return None

    try:
        client = OpenAI(api_key=api_key)
    except Exception as e:
        print(f"[insights] OpenAI error: {e}", file=sys.stderr)
        return None

    retries = 7
    attempted = 0
    for try_i in range(retries):
        attempted = try_i + 1
        try:
            resp = client.chat.completions.create(
                model="gpt-5",
                messages=[
                    {"role": "system", "content": "You are GeneSage. Return concise, publication-ready Markdown."},
                    {"role": "user", "content": prompt},
                ],
                temperature=temperature,
                max_completion_tokens=max_output_tokens or 10048,
                timeout=timeout_seconds,
            )
            text = resp.choices[0].message.content if resp and resp.choices else None
            if text and text.strip():
                return text
            print(f"[insights] OpenAI empty response; retrying ({attempted}/{retries})", file=sys.stderr)
            try:
                print(f"[insights] Raw response: {resp}", file=sys.stderr)
            except Exception:
                pass
            if try_i < retries - 1:
                _sleep_backoff(try_i)
            continue
        except Exception as e:
            print(f"[insights] OpenAI error: {e} ({attempted}/{retries})", file=sys.stderr)
            if _is_rate_or_quota_error(e) or _is_transient_server_error(e):
                if try_i < retries - 1:
                    _sleep_backoff(try_i)
                    continue
            break
    print(f"[insights] OpenAI failed after {attempted} attempts", file=sys.stderr)
    return None


def _safe_generate_with_fallbacks(prompt: str,
                                  temperature: float = 1,
                                  max_output_tokens: Optional[int] = None,
                                  timeout_seconds: int = 60,
                                  per_model_retries: int = 4) -> str:
    """Try multiple models with retries/backoff; always return markdown."""
    # Prefer OpenAI when available
    text = _try_openai(prompt, temperature, max_output_tokens, timeout_seconds)
    if text:
        return to_markdown(text)

    prompt_tokens = _approx_tokens(prompt)
    candidates = _eligible_models_for_prompt(prompt_tokens)

    # Optional limit: cap to each model's supported output
    def _cap_out(model: str) -> int:
        limit = MODEL_REGISTRY.get(model, {}).get("out", 2048)
        # Keep some headroom for safety; you can tune this.
        return min(limit, max_output_tokens or limit)

    last_error: Optional[str] = None

    for model_name in candidates:
        # Skip models that are obviously too small for the prompt (if computed)
        if MODEL_REGISTRY.get(model_name, {}).get("in", 0) < int(prompt_tokens * 1.10):
            # We'll still attempt, but log a note for visibility.
            print(f"[insights] Note: {model_name} may be too small for {prompt_tokens} tokens.", file=sys.stderr)

        # Build model with a tiny system instruction to bias format (optional)
        try:
            model = genai.GenerativeModel(
                model_name=model_name,
                system_instruction="You are GeneSage. Return concise, publication-ready Markdown."
            )
        except Exception as e:
            last_error = f"init {model_name}: {e}"
            print(f"[insights] Failed to init {model_name}: {e}", file=sys.stderr)
            continue

        for try_i in range(per_model_retries + 1):
            try:
                final_response = model.generate_content(
                    prompt,
                    generation_config={
                        "temperature": temperature,
                        "top_p": 0.95,
                        "max_output_tokens": _cap_out(model_name),
                    },
                    request_options={"timeout": timeout_seconds},
                )
                text = _extract_text(final_response)
                if text and text.strip():
                    return to_markdown(text)
                else:
                    # Empty text but no exception — try next model.
                    last_error = f"{model_name}: empty response"
                    print(f"[insights] {last_error}", file=sys.stderr)
                    break
            except Exception as e:
                err = str(e)
                last_error = f"{model_name} try {try_i}: {err}"
                # Handle common failure modes
                if _is_prompt_too_long_error(e):
                    print(f"[insights] {model_name} rejected prompt as too long; trying a larger-context model.", file=sys.stderr)
                    # Go to next candidate (bigger context).
                    try_i = per_model_retries  # force break
                    break
                if _is_rate_or_quota_error(e) or _is_transient_server_error(e):
                    print(f"[insights] Transient/429 on {model_name}; backoff and retry… ({try_i+1}/{per_model_retries})", file=sys.stderr)
                    if try_i < per_model_retries:
                        _sleep_backoff(try_i)
                        continue
                else:
                    print(f"[insights] Non-retryable error on {model_name}: {e}", file=sys.stderr)
                # On non-retryable or exhausted retries, break to next model
                break

    # If we got here, all candidates failed — return graceful markdown.
    msg = "# Biology insights\nNo biology insights available."
    if last_error:
        msg += f"\n\n> _Reason_: {last_error}"
    return msg
# NEW: second-pass formatter to harmonize cluster write-ups
def harmonize_biology_markdown(leiden: str, dataset_name: str, raw_md: str) -> str:
    """Reformat mixed per-cluster outputs into a strict, uniform Markdown template."""
    template = f"""Use this exact block for every cluster:

### {leiden} <cluster_id>
Canonical Label: <Title Case>
Fine Label: <short subtype or N/A>
Confidence: High | Medium | Low
Key Markers: <HGNC symbols only, comma-separated, 3–8 items>

<2–4 sentence summary that uses only inputs already mentioned in the source. No invented stats.>"""

    prompt = f"""
ROLE
You are GeneSage. Reformat the cluster summaries into harmonised Markdown.

RULES
- Preserve dataset name ({dataset_name}) and the exact cluster IDs (e.g., A1, A10) and their order.
- One block per cluster, no extra commentary.
- Fields must appear exactly as in the template, same order and spelling.
- "Confidence" must be one of: High, Medium, Low.
- "Key Markers": use official HGNC symbols only (strip any aliases in parentheses), 3–8 items, comma-separated.
- If a field is missing or unclear, write "N/A".
- Do not invent numbers or p-values that weren’t present in the source text.
- Output ONLY the harmonised Markdown blocks (no fences, no prose).

TEMPLATE
{template}

INPUT (mixed cluster paragraphs):
---
{raw_md}
---
OUTPUT
Return only the harmonised Markdown.
"""
    out = _safe_generate_with_fallbacks(prompt, temperature=1, max_output_tokens=6400)
    return (out or "").strip() or raw_md


def generate_biology_insights(
    leiden: str,
    dataset_name: str,
    cluster_sizes_csv: str,
    top_markers_json: str,
    markers_json: str,
    ctype_rank_csv: str,
    tf_markers_json: str,
    tissue_context: str,
    progress_cb: Optional[Callable[[int, str, int], None]] = None,
) -> str:
    """Generate cluster-wise cell-type predictions and compile a report.

    The function loops over each category in ``leiden`` and queries the
    language model separately for every cluster.  ``top_markers_json`` may
    contain RNA-only entries or separate RNA/Protein dictionaries.  Marker
    lists are expected to be ranked by ``Score`` where positive values indicate
    enrichment and negative values indicate depletion.  The individual outputs
    are concatenated into a single markdown string for display in the biology
    insights tab.

    ``progress_cb`` is an optional callback invoked before each model query
    with the signature ``callback(index, cluster_id, total)``.  It can be used
    to expose real-time progress information to the caller.
    """

    cluster_sizes = pd.read_csv(io.StringIO(cluster_sizes_csv))
    top_markers = json.loads(top_markers_json)
    markers = json.loads(markers_json)
    ctype_rank = pd.read_csv(io.StringIO(ctype_rank_csv))
    tf_markers = json.loads(tf_markers_json)

    batch_size = 5
    batch: list[str] = []
    harmonized_blocks: list[str] = []
    clusters = cluster_sizes["cluster"].astype(str)
    total = len(clusters)

    # Prepare prompts for all clusters while invoking progress callback
    cluster_prompts: list[tuple[str, str]] = []
    for i, cluster in enumerate(clusters, 1):
        if progress_cb:
            progress_cb(i, cluster, total)
        size = int(
            cluster_sizes.loc[
                cluster_sizes["cluster"].astype(str) == cluster, "n_cells"
            ].iloc[0]
        )

        if isinstance(top_markers, dict) and "RNA" in top_markers:
            cl_top_markers_rna = json.dumps(top_markers.get("RNA", {}).get(cluster, []))
            cl_top_markers_prot = json.dumps(top_markers.get("Protein", {}).get(cluster, []))
            top_marker_lines = (
                "Top RNA markers ranked by Score (positive=enrichment, negative=depletion) (JSON): "
                f"{cl_top_markers_rna}\n"
                "Top protein markers ranked by Score (positive=enrichment, negative=depletion) (JSON): "
                f"{cl_top_markers_prot}"
            )
        else:
            cl_top_markers_rna = json.dumps(top_markers.get(cluster, []))
            top_marker_lines = (
                "Neighborhood-aware marker genes ranked by Score (positive=enrichment, negative=depletion) (JSON): "
                f"{cl_top_markers_rna}"
            )

        cl_markers = json.dumps({m: d.get(cluster, []) for m, d in markers.items()})
        cl_ctype_csv = ctype_rank[ctype_rank["cluster"].astype(str) == cluster].to_csv(
            index=False
        )
        cl_tf_json = json.dumps(tf_markers.get(cluster, []))

        prompt = f"""
ROLE
You are “GeneSage,” a PhD-level molecular biologist and single-cell bioinformatics specialist.

OBJECTIVE
Predict the most likely cell type for {leiden} cluster {cluster} in dataset {dataset_name} using ONLY the inputs provided.

INPUTS
Cluster size: {size}
{top_marker_lines}
Additional marker genes per method (JSON): {cl_markers}
Cell-type enrichment from PanglaoDB (CSV): {cl_ctype_csv}
TF programs (JSON): {cl_tf_json}
Tissue context: {tissue_context}

OUTPUT
Return a concise markdown paragraph with the canonical label, an optional fine
label, confidence (High/Medium/Low), and key markers supporting the call.
"""

        cluster_prompts.append((cluster, prompt))

    # Dispatch model queries concurrently
    def _gen(args: tuple[str, str]) -> tuple[str, str]:
        cl, pr = args
        return cl, _safe_generate_with_fallbacks(pr) or ""

    max_workers = min(4, total) or 1
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        for cluster, result in ex.map(_gen, cluster_prompts):
            batch.append(f"### {leiden} {cluster}\n{to_markdown(result)}")
            if len(batch) >= batch_size:
                block_raw = "\n\n".join(batch)
                try:
                    harmonized_blocks.append(
                        harmonize_biology_markdown(leiden, dataset_name, block_raw)
                    )
                except Exception as e:
                    print(f"[insights] harmonisation failed: {e}", file=sys.stderr)
                    harmonized_blocks.append(block_raw)
                batch = []

    if batch:
        block_raw = "\n\n".join(batch)
        try:
            harmonized_blocks.append(
                harmonize_biology_markdown(leiden, dataset_name, block_raw)
            )
        except Exception as e:
            print(f"[insights] harmonisation failed: {e}", file=sys.stderr)
            harmonized_blocks.append(block_raw)

    return "\n\n".join(harmonized_blocks)


def inline_vendor_assets(html_path: str = "template_pseudo.html") -> None:
    """Embed external CDN JS/CSS directly into the HTML template.

    The function searches for ``<script src="...">`` and ``<link href="...">``
    tags that reference ``http`` or ``https`` resources, downloads each asset
    only once, and replaces the tag with an inline ``<script>`` or ``<style>``
    block containing the fetched, already‑minified code.  This produces a single
    self‑contained HTML file that renders without internet access.

    Parameters
    ----------
    html_path:
        Path to the HTML template to be processed in place.
    """

    import pathlib
    import re
    import requests
    from concurrent.futures import ThreadPoolExecutor

    tpl = pathlib.Path(html_path)
    html = tpl.read_text(encoding="utf-8")
    delim = SecTemplate.delimiter
    cache: dict[str, str] = {}

    def fetch(url: str) -> str:
        if url not in cache:
            res = requests.get(url, headers={"Accept-Encoding": "gzip, deflate"})
            res.raise_for_status()
            cache[url] = res.text.replace(delim, delim * 2)
        return cache[url]

    script_pat = re.compile(r'<script[^>]*src="(https?://[^"]+)"[^>]*></script>')
    link_pat = re.compile(r'<link[^>]*href="(https?://[^"]+)"[^>]*>')
    urls = set(script_pat.findall(html) + link_pat.findall(html))
    with ThreadPoolExecutor(max_workers=min(8, len(urls) or 1)) as ex:
        list(ex.map(fetch, urls))

    html = script_pat.sub(lambda m: f"<script>\n{cache[m.group(1)]}\n</script>", html)
    html = link_pat.sub(lambda m: f"<style>\n{cache[m.group(1)]}\n</style>", html)

    tpl.write_text(html, encoding="utf-8")


if __name__ == "__main__":
    inline_vendor_assets()
# Pseudobulk functions appended
EDGE_R_SCRIPT = """#!/usr/bin/env Rscript
# EdgeR pipeline script omitted for brevity; should contain differential expression workflow.
"""


def write_pseudobulk_and_metadata(
    adata,
    celltype_var="celltype",
    Individual_var="individual",
    Group_var="disease",
    outdir="Pseudobulk",
):
    """Generate pseudobulk count and metadata CSVs for each cell type."""
    os.makedirs(outdir, exist_ok=True)
    cell_types = adata.obs[celltype_var].unique().tolist()
    for celltype in cell_types:
        bdata = adata[adata.obs[celltype_var] == celltype, :].copy()
        bdata.X = bdata.layers["counts"]
        bdata.obs["clustersample"] = (
            bdata.obs[Individual_var].astype(str)
            + "_"
            + bdata.obs[celltype_var].astype(str)
        )
        bdata.obs["clustersample"] = bdata.obs["clustersample"].astype("category")
        res = pd.DataFrame(
            columns=bdata.var_names,
            index=bdata.obs["clustersample"].cat.categories,
        )
        for clust in bdata.obs["clustersample"].cat.categories:
            res.loc[clust] = bdata[bdata.obs["clustersample"] == clust, :].X.sum(0)
        counts_file = os.path.join(outdir, f"{celltype}_pseudobulk_counts.csv")
        res.T.to_csv(counts_file)
        metadata = (
            bdata.obs
            .drop_duplicates(subset=["clustersample"])[["clustersample", celltype_var, Individual_var, Group_var]]
            .rename(columns={"clustersample": "Sample"})
        )
        metadata_file = os.path.join(outdir, f"{celltype}_metadata.csv")
        metadata.to_csv(metadata_file, index=False)


def _detect_cols(df):
    cols_lower = {c.lower(): c for c in df.columns}
    for k in ("logfc", "log2fc", "log2foldchange"):
        if k in cols_lower:
            logfc_col = cols_lower[k]
            break
    else:
        raise ValueError("Could not find a logFC column")
    for k in ("fdr", "adj.p.val", "padj", "fdr_bh", "qvalue"):
        if k in cols_lower:
            fdr_col = cols_lower[k]
            break
    else:
        for k in ("pvalue", "p.value"):
            if k in cols_lower:
                fdr_col = cols_lower[k]
                break
        else:
            raise ValueError("Could not find an FDR/Padj column")
    return logfc_col, fdr_col


def run_edger_batch(
    folder=".",
    rscript_path="EdgeR_pipeline.R",
    counts_pattern="*_pseudobulk_counts.csv",
    metadata_suffix="_metadata.csv",
    sample_col="Sample",
    group_col="disease",
    donor_col=None,
    relevel=None,
    contrast_groups=None,
    output_dir="edger_results",
    check_consistency=True,
    require_both_groups=True,
    fdr_thresh=0.05,
    lfc_thresh_abs=0.0,
):
    os.makedirs(output_dir, exist_ok=True)
    runs = []
    counts_files = sorted(glob.glob(os.path.join(folder, counts_pattern)))
    for cf in counts_files:
        celltype = os.path.basename(cf).replace("_pseudobulk_counts.csv", "")
        celltype_safe = re.sub(r"[^A-Za-z0-9._-]+", "_", celltype)
        mf = cf.replace("_pseudobulk_counts.csv", metadata_suffix)
        if not os.path.exists(mf):
            runs.append({"celltype": celltype, "counts": cf, "metadata": mf,
                         "status": "metadata_missing", "result_file": None,
                         "up_n": None, "down_n": None,
                         "stdout": "", "stderr": ""})
            continue
        if check_consistency:
            try:
                mdf = pd.read_csv(mf)
                need = [sample_col, group_col] + ([donor_col] if donor_col else [])
                for col in need:
                    if col and col not in mdf.columns:
                        raise ValueError(f"metadata missing column: {col}")
                if contrast_groups and require_both_groups:
                    g1, g2 = contrast_groups
                    have = set(mdf[group_col].dropna().unique())
                    if not ({g1, g2} <= have):
                        raise ValueError(f"need {g1},{g2}; have {sorted(have)}")
                with open(cf, "r") as f:
                    header = f.readline().rstrip("\n").split(",")
                count_cols = header[1:] if header and header[0].lower() in ("gene", "genes", "feature", "symbol") else header
                missing = sorted(set(mdf[sample_col].astype(str).unique()) - set(count_cols))
                if missing:
                    raise ValueError(f"samples not in counts: {missing[:5]}...")
            except Exception as e:
                runs.append({"celltype": celltype, "counts": cf, "metadata": mf,
                             "status": f"consistency_error: {e}", "result_file": None,
                             "up_n": None, "down_n": None,
                             "stdout": "", "stderr": ""})
                continue
        cf_abs, mf_abs = os.path.abspath(cf), os.path.abspath(mf)
        cmd = ["Rscript", rscript_path, cf_abs, mf_abs, sample_col, group_col]
        if donor_col:
            cmd.append(donor_col)
        env = os.environ.copy()
        if relevel:
            env["RELEVEL"] = str(relevel)
        if contrast_groups:
            g1, g2 = contrast_groups
            env["CONTRAST_GROUPS"] = f"{g1},{g2}"
        result = subprocess.run(cmd, capture_output=True, text=True, env=env, cwd=folder)
        contrast_label = None
        for line in result.stdout.splitlines():
            if line.startswith("CONTRAST_LABEL="):
                contrast_label = line.split("=", 1)[1].strip()
                break
        if not contrast_label:
            contrast_label = "unknown"
        default_file = os.path.join(folder, f"{contrast_label}_edgeR_results.csv")
        target = os.path.join(output_dir, f"{contrast_label}_{celltype_safe}_edgeR_results.csv")
        up_n = down_n = None
        if os.path.exists(default_file):
            os.replace(default_file, target)
            try:
                df = pd.read_csv(target)
                logfc_col, fdr_col = _detect_cols(df)
                sig = df[df[fdr_col] <= fdr_thresh]
                if lfc_thresh_abs == 0:
                    up_n = int((sig[logfc_col] >= 0).sum())
                    down_n = int((sig[logfc_col] < 0).sum())
                else:
                    up_n = int(((sig[logfc_col].abs() >= lfc_thresh_abs) & (sig[logfc_col] > 0)).sum())
                    down_n = int(((sig[logfc_col].abs() >= lfc_thresh_abs) & (sig[logfc_col] < 0)).sum())
            except Exception as e:
                result.stderr += f"\n[summary_count_error] {e}"
            status = "ok" if result.returncode == 0 else f"error_{result.returncode}"
            result_file = target
        else:
            status = f"missing_output_{result.returncode}"
            result_file = None
        runs.append({"celltype": celltype, "counts": cf, "metadata": mf,
                     "status": status, "result_file": result_file,
                     "up_n": up_n, "down_n": down_n,
                     "stdout": result.stdout, "stderr": result.stderr})
    return pd.DataFrame(runs)


def plot_deg_counts(summary_df, figsize=(8,6), palette=("firebrick", "royalblue")):
    df = summary_df.dropna(subset=["up_n", "down_n"]).copy()
    df = df.sort_values("celltype")
    plot_df = df.melt(id_vars="celltype", value_vars=["up_n", "down_n"],
                      var_name="Direction", value_name="Count")
    plot_df["Direction"] = plot_df["Direction"].map({"up_n": "Upregulated", "down_n": "Downregulated"})
    sns.set_theme(style="white")
    fig, ax = plt.subplots(figsize=figsize)
    sns.scatterplot(
        data=plot_df,
        x="celltype",
        y="Count",
        hue="Direction",
        palette=palette,
        s=120,
        edgecolor="black",
        linewidth=0.6,
        ax=ax,
    )
    ax.set_xlabel("")
    ax.set_ylabel("Number of DE genes", fontsize=14, weight="bold")
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha="right", fontsize=12)
    ax.set_yticklabels(ax.get_yticks(), fontsize=12)
    ax.spines["top"].set_visible(False)
    ax.spines["right"].set_visible(False)
    ax.spines["left"].set_linewidth(1.2)
    ax.spines["bottom"].set_linewidth(1.2)
    ax.legend(frameon=False, fontsize=12, title="")
    ax.set_title("Differentially Expressed Genes by Cell Type", fontsize=16, weight="bold", pad=15)
    sns.despine()
    buf = io.BytesIO()
    fig.savefig(buf, format="png", bbox_inches="tight")
    plt.close(fig)
    return base64.b64encode(buf.getvalue()).decode("ascii")
